{"cells":[{"cell_type":"code","execution_count":1,"id":"4810a5ba","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4810a5ba","executionInfo":{"status":"ok","timestamp":1684837519716,"user_tz":-210,"elapsed":1870,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}},"outputId":"5f12d383-b511-4bb9-a37c-f9a41ab32296"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Year  2D-barcode  VIDEOMICROSCOPY  computational pathology  \\\n","0   1990           0                1                        0   \n","1   1991           0                2                        0   \n","2   1992           0                4                        0   \n","3   1993           0               12                        0   \n","4   1994           0                5                        0   \n","5   1995           0               13                        0   \n","6   1996           0                0                        0   \n","7   1997           0                0                        0   \n","8   1998           0                0                        0   \n","9   1999           0                0                        0   \n","10  2000           0                0                        0   \n","11  2002           0                0                        0   \n","12  2003           0                0                        0   \n","13  2004           0                0                        0   \n","14  2005           0                0                        0   \n","15  2006           0                0                        0   \n","16  2007           0                0                        0   \n","17  2008           0                0                        0   \n","18  2009           0                0                        0   \n","19  2010           0                0                        0   \n","20  2011           0                0                        0   \n","21  2012           0                0                        0   \n","22  2013           0                0                        0   \n","23  2014           0                0                        0   \n","24  2015           0                0                        0   \n","25  2016           0                0                        0   \n","26  2017           0                0                        0   \n","27  2018           0                0                        0   \n","28  2019           0                0                        4   \n","29  2020           0                0                        5   \n","30  2021           1                0                       10   \n","31  2022           0                0                       10   \n","\n","    convolutional neural network  deep learning  digital imaging  \\\n","0                              0              0                0   \n","1                              0              0                0   \n","2                              0              0                0   \n","3                              0              0                0   \n","4                              0              0                0   \n","5                              0              0                0   \n","6                              0              0                1   \n","7                              0              0                1   \n","8                              0              0                1   \n","9                              0              0                3   \n","10                             0              0                2   \n","11                             0              0                1   \n","12                             0              0                1   \n","13                             0              0                2   \n","14                             0              0                0   \n","15                             0              0                2   \n","16                             0              0                0   \n","17                             0              0                0   \n","18                             0              0                0   \n","19                             0              0                2   \n","20                             0              0                1   \n","21                             0              0                3   \n","22                             0              0                1   \n","23                             0              0                2   \n","24                             0              0                0   \n","25                             0              1                1   \n","26                             0              0                1   \n","27                             1              2                2   \n","28                             0             11                0   \n","29                            10             25                0   \n","30                             7             42                0   \n","31                             8             44                1   \n","\n","    digital pathology  image processing  imaging  machine learning  \\\n","0                   0                 0        0                 0   \n","1                   0                 0        0                 0   \n","2                   0                 0        0                 0   \n","3                   0                 0        0                 0   \n","4                   0                 0        0                 0   \n","5                   0                 0        0                 0   \n","6                   0                 0        0                 0   \n","7                   0                 0        0                 0   \n","8                   0                 2        0                 0   \n","9                   0                 0        1                 0   \n","10                  0                 2        0                 0   \n","11                  0                 1        0                 0   \n","12                  0                 2        1                 0   \n","13                  0                 3        0                 0   \n","14                  0                 1        0                 0   \n","15                  2                 1        2                 0   \n","16                  1                 0        0                 0   \n","17                  2                 2        1                 0   \n","18                  4                 1        0                 0   \n","19                  2                 2        0                 0   \n","20                  3                 2        0                 0   \n","21                  9                 1        0                 0   \n","22                  6                 1        1                 1   \n","23                 13                 0        0                 0   \n","24                  8                 1        0                 1   \n","25                 11                 1        0                 0   \n","26                 23                 2        0                 1   \n","27                 25                 1        2                 3   \n","28                 47                 0        0                 5   \n","29                 49                 2        2                12   \n","30                104                 3        1                20   \n","31                 97                 2        1                22   \n","\n","    neural network  \n","0                0  \n","1                0  \n","2                0  \n","3                0  \n","4                0  \n","5                0  \n","6                0  \n","7                0  \n","8                0  \n","9                0  \n","10               0  \n","11               1  \n","12               0  \n","13               0  \n","14               1  \n","15               0  \n","16               0  \n","17               0  \n","18               0  \n","19               0  \n","20               0  \n","21               0  \n","22               0  \n","23               0  \n","24               0  \n","25               0  \n","26               0  \n","27               0  \n","28               0  \n","29               0  \n","30               0  \n","31               1  \n"]}],"source":["import pandas as pd\n","\n","# Load the CSV file into a DataFrame\n","df = pd.read_excel('/content/Sample Data.xlsx', index_col=0)\n","\n","# Extract the 'Year' and 'Keywords' columns\n","df = df[['Year', 'keywords']]\n","\n","# Drop rows with NaN values in the 'Keywords' column\n","df = df.dropna(subset=['keywords'])\n","\n","# Convert the keywords column to string type\n","df['keywords'] = df['keywords'].astype(str)\n","\n","# Split the keywords column by '|' and create a list of keywords for each row\n","df['keywords'] = df['keywords'].str.split('|')\n","\n","# Create a list of keywords to search for\n","keywords_to_search = ['deep learning','imaging', 'machine learning','convolutional neural network','computational pathology', 'digital pathology','VIDEOMICROSCOPY', '2D-barcode', 'digital imaging', 'image processing', 'neural network']\n","# Create a boolean mask for rows that contain the keywords\n","mask = df['keywords'].apply(lambda x: any(keyword in x for keyword in keywords_to_search))\n","\n","# Apply the boolean mask to filter the rows\n","df_filtered = df[mask]\n","\n","# Explode the keywords column to create separate rows for each keyword\n","df_filtered = df_filtered.explode('keywords')\n","\n","# Group by year and keyword, and count the occurrences\n","result = df_filtered.groupby(['Year', 'keywords']).size().unstack(fill_value=0)\n","\n","# Reset the index to make years as columns and keywords as rows\n","result = result.reset_index()\n","\n","# Rename the columns for readability\n","result.columns.name = None\n","\n","# Set the 'Year' column as the index\n","result.set_index('Year', inplace=True)\n","\n","# Filter the resulting DataFrame to only include the keywords in keywords_to_search\n","result = result[result.columns.intersection(keywords_to_search)]\n","# Reset the index to make years as columns and keywords as rows\n","result = result.reset_index()\n","# Print the resulting DataFrame\n","print(result)\n","\n"]},{"cell_type":"code","execution_count":2,"id":"62cb2dd7","metadata":{"id":"62cb2dd7","executionInfo":{"status":"ok","timestamp":1684837519717,"user_tz":-210,"elapsed":26,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["# Save the filtered DataFrame as an Excel file\n","##result.to_excel('filtered_results.xlsx', index=False)"]},{"cell_type":"markdown","id":"95ae26fe","metadata":{"id":"95ae26fe"},"source":["Instead of saving records and add merged columns, you can run the below code. \n","Just remember to add every set of merged terms as a list such as ['ML','Machine Learning','machine learning']\n","\n","p.s. note that this code is case sensitive (ML is not ml) so check the errors if occurder for checking them... "]},{"cell_type":"code","execution_count":3,"id":"8e86929a","metadata":{"id":"8e86929a","executionInfo":{"status":"error","timestamp":1684837519718,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}},"colab":{"base_uri":"https://localhost:8080/","height":380},"outputId":"7dac8e17-218f-4945-a65e-246603a80a32"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-0878e6c930b6>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMerged\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum_reduced_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-0878e6c930b6>\u001b[0m in \u001b[0;36msum_reduced_keys\u001b[0;34m(df, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msum_reduced_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6129\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6130\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Deep Learning', 'Deep learning'], dtype='object')] are in the [columns]\""]}],"source":["Merged = [\n","        ['Deep Learning','Deep learning'],\n","        ['Machine Learning','machine learning']\n","         ]\n","\n","def sum_reduced_keys(df,x):\n","    name = x[0]\n","    new = df[x].sum(axis=1)\n","    df.drop(columns=x,inplace=True)\n","    df[name]=new\n","\n","for x in Merged: sum_reduced_keys(result,x)\n","    \n","new_df = result"]},{"cell_type":"code","execution_count":null,"id":"b34c70a8","metadata":{"id":"b34c70a8","executionInfo":{"status":"aborted","timestamp":1684837519718,"user_tz":-210,"elapsed":23,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["##new_df = pd.read_excel('filtered_results.xlsx', index_col=0)"]},{"cell_type":"code","execution_count":null,"id":"fae0303b","metadata":{"id":"fae0303b","executionInfo":{"status":"aborted","timestamp":1684837519719,"user_tz":-210,"elapsed":24,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import numpy as np\n","# Calculate CAGR for each keyword\n","result_cagr = pd.DataFrame(index=new_df.columns, columns=['CAGR'])\n","for keyword in new_df.columns:\n","    start_value = new_df[keyword].loc[new_df[keyword].ne(0).idxmax()]  # First non-zero value\n","    end_value = new_df[keyword].iloc[-1]  # Value in the last row\n","    n = new_df[keyword].ne(0).sum()  # Number of non-zero values\n","    cagr = (end_value / start_value)**(1 / n) - 1  # Calculate CAGR\n","    result_cagr.at[keyword, 'CAGR'] = cagr if cagr != np.inf else np.nan\n","\n","# Calculate share of keywords\n","result_share = pd.DataFrame(index=df.columns, columns=['Share'])\n","for keyword in new_df.columns:\n","    last_two_years_sum = new_df[keyword].iloc[-2:].sum()  # Sum of values for last two years\n","    total_sum = new_df[keyword].sum()  # Sum of all years\n","    share = (last_two_years_sum / total_sum) * 100  # Calculate share as percentage\n","    result_share.at[keyword, 'Share'] = share\n","\n","    # Calculate average of shares\n","avg_share = result_share['Share'].mean()\n","\n","# Create a new column to store quartile information\n","result_share['Quartile'] = ''\n","result_cagr['Quartile'] = ''\n","\n","# Loop through each keyword\n","for keyword in new_df.columns:\n","    share = result_share.at[keyword, 'Share']\n","    cagr = result_cagr.at[keyword, 'CAGR']\n","\n","    if share > avg_share:\n","        if cagr >= 0:\n","            result_share.at[keyword, 'Quartile'] = 'Dominant Technologies'\n","            result_cagr.at[keyword, 'Quartile'] = 'Dominant Technologies'\n","        else:\n","            result_share.at[keyword, 'Quartile'] = 'Saturated Technologies'\n","            result_cagr.at[keyword, 'Quartile'] = 'Saturated Technologies'\n","    else:\n","        if cagr > 0:\n","            result_share.at[keyword, 'Quartile'] = 'Emerging Technologies'\n","            result_cagr.at[keyword, 'Quartile'] = 'Emerging Technologies'\n","        else:\n","            result_share.at[keyword, 'Quartile'] = 'Declining Technologies'\n","            result_cagr.at[keyword, 'Quartile'] = 'Declining Technologies'\n","\n","# Calculate sum of frequency for node size\n","result_freq_sum = new_df.sum()"]},{"cell_type":"code","execution_count":null,"id":"a18f5f09","metadata":{"id":"a18f5f09","executionInfo":{"status":"aborted","timestamp":1684837519719,"user_tz":-210,"elapsed":24,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["# Print CAGR results\n","print(\"CAGR Results:\")\n","print(result_cagr)\n","\n","# Print Share results\n","print(\"Share Results:\")\n","print(result_share)\n"]},{"cell_type":"code","execution_count":null,"id":"973792c0","metadata":{"id":"973792c0","executionInfo":{"status":"aborted","timestamp":1684837519719,"user_tz":-210,"elapsed":24,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Create a figure and axes\n","fig, ax = plt.subplots(figsize=(8, 6))  # Update figsize to desired width and height in inches\n","\n","# Define colors for each quartile\n","colors = {\n","    'Emerging Technologies': 'green',\n","    'Dominant Technologies': 'blue',\n","    'Saturated Technologies': 'red',\n","    'Declining Technologies': 'gray'\n","}\n","\n","# Loop through each keyword\n","for keyword in new_df.columns:\n","    share = result_share.at[keyword, 'Share']\n","    cagr = result_cagr.at[keyword, 'CAGR']\n","    freq_sum = result_freq_sum.at[keyword]\n","\n","    # Get the quartile for the keyword\n","    quartile = result_cagr.at[keyword, 'Quartile']\n","\n","    # Get the color and size based on the quartile and frequency sum\n","    color = colors[quartile]\n","    size = freq_sum \n","\n","    # Plot the keyword as a scatter point with label\n","    ax.scatter(share, cagr, c=color, s=size, label=keyword)\n","\n","    # Add text label for each keyword\n","    ax.annotate(keyword, (share, cagr), textcoords=\"offset points\", xytext=(0, 10), ha='center', va='bottom', fontsize=8)\n","    \n","# Calculate mean of x-axis and plot vertical line\n","mean_share = new_df.sum(axis=1).mean()\n","ax.axvline(mean_share, color='black', linestyle='--', label='Mean Share')\n","\n","# Plot horizontal line for CAGR = 0\n","ax.axhline(0, color='gray', linestyle='--', label='CAGR = 0')\n","\n","# Set labels and title\n","ax.set_xlabel('Share of Keywords')\n","ax.set_ylabel('CAGR')\n","ax.set_title('Keyword Analysis')\n","\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"90dc86a0","metadata":{"id":"90dc86a0","executionInfo":{"status":"aborted","timestamp":1684837519720,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["mean_share"]},{"cell_type":"code","source":["! pip install pymannkendall"],"metadata":{"id":"EXnOkeTpv1gp","executionInfo":{"status":"aborted","timestamp":1684837519720,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"id":"EXnOkeTpv1gp","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"aac551d3","metadata":{"id":"aac551d3","executionInfo":{"status":"aborted","timestamp":1684837519721,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib as plt\n","import pymannkendall as mk\n","import numpy as np\n","\n","# Create an empty DataFrame to store the results\n","results_df = pd.DataFrame(columns=['Keyword', 'Test Statistic', 'p-value', 'Trend', 'h', 'z', 'Tau', 's', 'var_s', 'slope'])\n","\n","# Loop through each keyword column in the DataFrame\n","for col in new_df.columns:\n","    # Perform Mann-Kendall test on the column\n","    result = mk.original_test(new_df[col].values)\n","    \n","    # Extract the test statistic, p-value, and trend (if any)\n","    test_statistic = result[0]\n","    p_value = result[1]\n","    trend = result[2]\n","    h = result[3]\n","    z = result[4]\n","    Tau = result[5]\n","    s = result[6]\n","    var_s = result[7]\n","    slope = result[8]\n","\n","    \n","# Append the results to the DataFrame\n","    results_df = results_df.append({'Keyword': col, 'Test Statistic': test_statistic, 'p-value': p_value, 'Trend': trend, 'h': h, 'z': z, 'Tau': Tau, 's': s, 'var_s': var_s, 'slope': slope}, ignore_index=True)\n","\n","# Print the results in tabular format\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"id":"7e550c95","metadata":{"id":"7e550c95","executionInfo":{"status":"aborted","timestamp":1684837519721,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","# Transpose the DataFrame to switch the years to x-axis and keywords to y-axis\n","df_transposed = new_df.T\n","\n","\n","# Set the plot size\n","plt.figure(figsize=(15, 6))\n","\n","# Create a heatmap using seaborn\n","sns.heatmap(df_transposed, cmap='YlGnBu', cbar_kws={'label': 'Frequency'}, annot=True, fmt=\"d\")\n","\n","# Set the title and labels\n","plt.title('Keyword Frequencies Heatmap')\n","plt.xlabel('Keywords')\n","plt.ylabel('Year')\n","\n","# Rotate x-axis labels for better visibility\n","plt.xticks(rotation=90)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"cb712082","metadata":{"id":"cb712082","executionInfo":{"status":"aborted","timestamp":1684837519722,"user_tz":-210,"elapsed":26,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","# Set plot style for lines\n","line_styles = ['-', '--', '-.', ':']  # Specify different line styles\n","num_styles = len(line_styles)\n","\n","# Set plot size\n","plt.figure(figsize=(10, 6))\n","\n","# Loop through each column (keyword) in the DataFrame and plot a line\n","for i, column in enumerate(new_df.columns):\n","    # Get x-axis (years) and y-axis (frequencies) data\n","    x = new_df.index\n","    y = new_df[column]\n","    \n","    # Plot the line with the specified style and label\n","    plt.plot(x, y, linestyle=line_styles[i % num_styles], label=column)\n","\n","# Set log scale for y-axis\n","plt.yscale('log')\n","\n","# Set title and labels\n","plt.title('Keyword Frequencies over Years (Log Scale)')\n","plt.xlabel('Year')\n","plt.ylabel('Frequency (log scale)')\n","\n","# Add legend\n","plt.legend()\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"6f47db9e","metadata":{"id":"6f47db9e","executionInfo":{"status":"aborted","timestamp":1684837519722,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","def klinsberg_burst_detection(values, threshold=0.5, min_burst_length=2):\n","    \"\"\"\n","    Detect bursts in a time series using the Kinsberg algorithm.\n","   \n","    Args:\n","        values (np.array): 1D numpy array of values.\n","        threshold (float): Threshold value for detecting bursts. Default is 3.\n","        min_burst_length (int): Minimum length of a burst to be considered. Default is 2.\n","   \n","    Returns:\n","        bursts (list): List of dictionaries containing 'Weight', 'Length', 'Start', and 'End' values for each burst.\n","    \"\"\"\n","    bursts = []\n","    n = len(values)\n","    mean = np.mean(values)\n","    std = np.std(values)\n","   \n","    for i in range(n):\n","        if values[i] > mean + threshold * std:\n","            start = i\n","            end = i\n","            weight = values[i] - mean\n","            length = 1\n","           \n","            # Extend the burst\n","            for j in range(i+1, n):\n","                if values[j] > mean + threshold * std and j == end + 1:\n","                    weight += values[j] - mean\n","                    end = j\n","                    length += 1\n","                else:\n","                    break\n","           \n","            # Check if the burst meets the minimum length requirement\n","            if length >= min_burst_length:\n","                burst = {'Weight': weight, 'Length': length, 'Start': start, 'End': end}\n","                bursts.append(burst)\n","   \n","    return bursts\n","\n","# Read the Excel file into a DataFrame\n","# new_df = pd.read_excel('filtered_results.xlsx', index_col=0)\n","\n","# Get all column names except for the 'Year' column\n","keywords = new_df.columns.tolist()[1:]\n","\n","# Create an empty DataFrame to store the results\n","results_df = pd.DataFrame(columns=['Keyword', 'Weight', 'Length', 'Start', 'End'])\n","\n","# Loop through each keyword and calculate bursts\n","for keyword in keywords:\n","    # Get the values for the current keyword\n","    values = new_df[keyword].values\n","   \n","    # Call the klinsberg_burst_detection function\n","    bursts = klinsberg_burst_detection(values)\n","   \n","    # Merge consecutive bursts within a certain threshold of years\n","    merged_bursts = []\n","    for burst in bursts:\n","        if len(merged_bursts) > 0 and burst['Start'] - merged_bursts[-1]['End'] <= 1:\n","            # If the current burst starts within 1 year of the previous burst ending, merge them\n","            merged_bursts[-1]['End'] = burst['End']\n","            merged_bursts[-1]['Length'] = burst['End'] - merged_bursts[-1]['Start'] + 1\n","            merged_bursts[-1]['Weight'] += burst['Weight']\n","        else:\n","            # Otherwise, add the burst as a new burst\n","            merged_bursts.append(burst)\n","   \n","    # Loop through each merged burst and append the results to the DataFrame\n","    for burst in merged_bursts:\n","        # Get the start and end indices from the burst dictionary\n","        start = new_df.index[burst['Start']]\n","        end = new_df.index[burst['End']]\n","       \n","        # Append the burst details to the DataFrame\n","        results_df = results_df.append({'Keyword': keyword,\n","                                        'Weight': burst['Weight'],\n","                                        'Length': burst['Length'],\n","                                        'Start': start,'End': end}, ignore_index=True)\n","#Sort the results by keyword, start date, and end date\n","\n","results_df.sort_values(by=['Keyword', 'Start', 'End'], inplace=True)\n","#Reset the index of the results DataFrame\n","\n","results_df.reset_index(drop=True, inplace=True)\n","#Print the final results\n","\n","print(results_df)"]},{"cell_type":"code","execution_count":null,"id":"5d3c98b8","metadata":{"id":"5d3c98b8","executionInfo":{"status":"aborted","timestamp":1684837519722,"user_tz":-210,"elapsed":25,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["# Save the results DataFrame to a CSV file\n","results_df.to_csv('burst_results.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"id":"02c7cd37","metadata":{"id":"02c7cd37","executionInfo":{"status":"aborted","timestamp":1684837519723,"user_tz":-210,"elapsed":26,"user":{"displayName":"Mohamad Reza Pazhouhan","userId":"01568694677342902940"}}},"outputs":[],"source":["# Load the dataset from CSV\n","df3 = pd.read_csv('burst_results.csv')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}